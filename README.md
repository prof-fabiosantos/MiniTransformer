# ğŸ§  Transformer Core

**Transformer Core** Ã© uma implementaÃ§Ã£o educacional e compacta da arquitetura Transformer, projetada para tarefas de NLP como geraÃ§Ã£o de texto e perguntas/respostas. Leve, interpretÃ¡vel e pronta para personalizaÃ§Ã£o, ela Ã© ideal para estudantes, entusiastas e experimentos locais.

<p align="center">
  <img src="./logo_minitransformer.png" alt="Logo Transformer Core" width="300"/>
</p>

---

## â„¹ï¸ O que Ã© um Transformer?

Transformer Ã© uma arquitetura de rede neural que mudou fundamentalmente a abordagem da InteligÃªncia Artificial. Foi introduzida no artigo seminal "Attention is All You Need" em 2017 e desde entÃ£o se tornou a principal arquitetura para modelos de deep learning, impulsionando modelos de geraÃ§Ã£o de texto como o GPT da OpenAI, o Llama da Meta e o Gemini do Google. AlÃ©m do texto, o Transformer tambÃ©m Ã© aplicado na geraÃ§Ã£o de Ã¡udio, reconhecimento de imagens, previsÃ£o de estrutura de proteÃ­nas e atÃ© em jogos, demonstrando sua versatilidade em diversos domÃ­nios.

Fundamentalmente, modelos Transformers de geraÃ§Ã£o de texto operam com base no princÃ­pio de previsÃ£o da prÃ³xima palavra: dado um prompt textual do usuÃ¡rio, qual Ã© a prÃ³xima palavra mais provÃ¡vel? A inovaÃ§Ã£o central e o poder dos Transformers residem no uso do mecanismo de autoatenÃ§Ã£o (self-attention), que permite processar sequÃªncias inteiras e capturar dependÃªncias de longo alcance com mais eficÃ¡cia do que arquiteturas anteriores.

A famÃ­lia GPT-2 Ã© um exemplo proeminente de Transformers para geraÃ§Ã£o de texto. O Transformer Core se inspira nesses modelos e compartilha muitos dos mesmos componentes e princÃ­pios arquiteturais fundamentais encontrados nos modelos atuais de ponta, tornando-o ideal para aprendizado e compreensÃ£o bÃ¡sica.

---

## ğŸ§¬ Arquitetura Transformer

Todo Transformer de geraÃ§Ã£o de texto Ã© composto por trÃªs componentes principais:

* **Embedding**: A entrada textual Ã© dividida em unidades menores chamadas *tokens*, que podem ser palavras ou subpalavras. Esses tokens sÃ£o convertidos em vetores numÃ©ricos chamados *embeddings*, que capturam o significado semÃ¢ntico das palavras.

* **Bloco Transformer**: Ã‰ o bloco fundamental do modelo que processa e transforma os dados de entrada. Cada bloco inclui:

  * **Mecanismo de AtenÃ§Ã£o (Attention Mechanism)**: permite que os tokens se comuniquem entre si, capturando informaÃ§Ãµes contextuais e relaÃ§Ãµes entre palavras.
  * **Camada MLP (Multilayer Perceptron)**: uma rede feed-forward que opera em cada token de forma independente. Enquanto a atenÃ§Ã£o roteia informaÃ§Ãµes entre tokens, o MLP refina a representaÃ§Ã£o de cada token.

* **Probabilidades de SaÃ­da**: As camadas finais lineares e softmax transformam os *embeddings* processados em probabilidades, permitindo que o modelo preveja o prÃ³ximo token na sequÃªncia.


---

## ğŸ§± Arquitetura do Transformer Core

O `Transformer Core` Ã© composto pelos seguintes blocos principais:

* **`Embedding`**: embedding de palavras e posiÃ§Ãµes
* **`TransformerBlock`**: bloco completo com LayerNorm, residual e atenÃ§Ã£o + MLP
* **`MultiHeadSelfAttention`**: atenÃ§Ã£o com projeÃ§Ã£o conjunta QKV, mÃ¡scara causal e concatenaÃ§Ã£o de cabeÃ§as
* **`TransformerMLP`**: MLP com GELU e projeÃ§Ãµes lineares
* **`SimpleTransformer`**: empilhamento de blocos, seguido por uma projeÃ§Ã£o linear para logits

Exemplo de configuraÃ§Ã£o:

```python
model = SimpleTransformer(
    vocab_size=len(vocab),
    embed_dim=128,
    num_heads=8,
    num_layers=4
)
```
---

## ğŸš€ Recursos

* Arquitetura Transformer simplificada com PyTorch puro
* Camada de atenÃ§Ã£o multi-cabeÃ§a (multi-head self-attention)
* ConexÃµes residuais com normalizaÃ§Ã£o e MLP
* ProjeÃ§Ãµes integradas de Q, K e V
* GeraÃ§Ã£o de texto com temperatura, top-k e top-p
* Dataset estilo QA baseado em texto plano

---

## ğŸ“ Estrutura do projeto

```bash
transformer_chatbot_project/
â”œâ”€â”€ model.py                # Arquitetura do Transformer Core
â”œâ”€â”€ train_transformer.py  # Script de treino e geraÃ§Ã£o
â”œâ”€â”€ textDataset.txt        # Dados de treinamento com pares QA
â”œâ”€â”€ modelo_transformer.pt  # Modelo treinado
â”œâ”€â”€ vocab_transformer.json # VocabulÃ¡rio serializado
â”œâ”€â”€ TransformerChatbotTreino.ipynb  # VersÃ£o notebook interativo
â””â”€â”€ transformer_app # Web app interativo

```

---

## ğŸ Como usar

```bash
pip install torch streamlit numpy
ou
pip install -r requirements.txt

# Treinar modelo
python train_transformer.py

# Rodar app Streamlit (caso implementado)
streamlit run transformer_app.py
```

---

## ğŸ“Œ ParÃ¢metros tÃ­picos

* Camadas: 4
* CabeÃ§as de atenÃ§Ã£o: 8
* Embedding: 128
* Hidden MLP: 256
* Tokens mÃ¡ximos por prompt: 100
* ParÃ¢metros totais: \~806.000

---

## âœ¨ Exemplo de uso

```text
prompt: pergunta: quanto Ã© 2 mais 2? resposta:
output: 4 <eos>
```

---

## ğŸ“š ReferÃªncias

* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
* Baseado em prÃ¡ticas de modelos como GPT-2

---

Desenvolvido para aprendizado, prototipagem e experimentaÃ§Ã£o.

> "Construa seu prÃ³prio modelo, compreenda cada atenÃ§Ã£o."
