# ğŸ§  Transformer Core

**Transformer Core** Ã© uma implementaÃ§Ã£o educacional e compacta do Decoder da arquitetura Transformer, projetada para tarefas de NLP como geraÃ§Ã£o de texto e perguntas/respostas. Leve, interpretÃ¡vel e pronta para personalizaÃ§Ã£o, ela Ã© ideal para estudantes, entusiastas e experimentos locais.

<p align="center">
  <img src="./logo_minitransformer.png" alt="Logo Transformer Core" width="300"/>
</p>

---

## â„¹ï¸ O que Ã© um Transformer?

Transformer Ã© uma arquitetura de rede neural que mudou fundamentalmente a abordagem da InteligÃªncia Artificial. Foi introduzida no artigo seminal "Attention is All You Need" em 2017 e desde entÃ£o se tornou a principal arquitetura para modelos de deep learning, impulsionando modelos de geraÃ§Ã£o de texto como o GPT da OpenAI, o Llama da Meta e o Gemini do Google. AlÃ©m do texto, o Transformer tambÃ©m Ã© aplicado na geraÃ§Ã£o de Ã¡udio, reconhecimento de imagens, previsÃ£o de estrutura de proteÃ­nas e atÃ© em jogos, demonstrando sua versatilidade em diversos domÃ­nios.

Fundamentalmente, modelos Transformers de geraÃ§Ã£o de texto operam com base no princÃ­pio de previsÃ£o da prÃ³xima palavra: dado um prompt textual do usuÃ¡rio, qual Ã© a prÃ³xima palavra mais provÃ¡vel? A inovaÃ§Ã£o central e o poder dos Transformers residem no uso do mecanismo de autoatenÃ§Ã£o (self-attention), que permite processar sequÃªncias inteiras e capturar dependÃªncias de longo alcance com mais eficÃ¡cia do que arquiteturas anteriores.

A famÃ­lia GPT-2 Ã© um exemplo proeminente do decoder da arquiterura Transformers para geraÃ§Ã£o de texto. O Transformer Core se inspira nesses modelos e compartilha muitos dos mesmos componentes e princÃ­pios arquiteturais fundamentais encontrados nos modelos atuais de ponta, tornando-o ideal para aprendizado e compreensÃ£o bÃ¡sica.

---

## ğŸ§¬ Arquitetura Transformer

Todo Decoder da arquitetura Transformer Ã© composto por trÃªs componentes principais:

* **Embedding**: A entrada textual Ã© dividida em unidades menores chamadas *tokens*, que podem ser palavras ou subpalavras. Esses tokens sÃ£o convertidos em vetores numÃ©ricos chamados *embeddings*, que capturam o significado semÃ¢ntico das palavras.

* **Bloco Transformer**: Ã‰ o bloco fundamental do modelo que processa e transforma os dados de entrada. Cada bloco inclui:

  * **Mecanismo de AtenÃ§Ã£o (Attention Mechanism)**: permite que os tokens se comuniquem entre si, capturando informaÃ§Ãµes contextuais e relaÃ§Ãµes entre palavras.
  * **Camada MLP (Multilayer Perceptron)**: uma rede feed-forward que opera em cada token de forma independente. Enquanto a atenÃ§Ã£o roteia informaÃ§Ãµes entre tokens, o MLP refina a representaÃ§Ã£o de cada token.

* **Probabilidades de SaÃ­da**: As camadas finais lineares e softmax transformam os *embeddings* processados em probabilidades, permitindo que o modelo preveja o prÃ³ximo token na sequÃªncia.


---

## ğŸ§± Arquitetura do Transformer Core

O `Transformer Core` Ã© composto pelos seguintes blocos principais:

* **`Embedding`**: embedding de palavras e posiÃ§Ãµes
* **`TransformerBlock`**: bloco completo com LayerNorm, residual e atenÃ§Ã£o + MLP
* **`MultiHeadSelfAttention`**: atenÃ§Ã£o com projeÃ§Ã£o conjunta QKV, mÃ¡scara causal e concatenaÃ§Ã£o de cabeÃ§as
* **`TransformerMLP`**: MLP com GELU e projeÃ§Ãµes lineares
* **`SimpleTransformer`**: empilhamento de blocos, seguido por uma projeÃ§Ã£o linear para logits

Exemplo de configuraÃ§Ã£o:

```python
model = SimpleTransformer(
    vocab_size=len(vocab),
    embed_dim=128,
    num_heads=8,
    num_layers=4
)
```
---

## ğŸš€ Recursos

* Arquitetura Transformer simplificada com PyTorch puro
* Camada de atenÃ§Ã£o multi-cabeÃ§a (multi-head self-attention)
* ConexÃµes residuais com normalizaÃ§Ã£o e MLP
* ProjeÃ§Ãµes integradas de Q, K e V
* GeraÃ§Ã£o de texto com temperatura, top-k e top-p
* Dataset estilo QA baseado em texto plano

---

## ğŸ“ Estrutura do projeto

```bash
transformer_core_project/
â”œâ”€â”€ transformer_app_playground   # App Playground
â”œâ”€â”€ transformer_train   # App Transforme Train 
â”œâ”€â”€ model.py                # Arquitetura do Transformer Core
â”œâ”€â”€ train_transformer.py  # Script de treino e geraÃ§Ã£o via terminal
â”œâ”€â”€ textDataset.txt        # Dados de treinamento com pares QA
â”œâ”€â”€ modelo_transformer.pt  # Modelo treinado
â”œâ”€â”€ vocab_transformer.json # VocabulÃ¡rio serializado
â”œâ”€â”€ TransformerCoreTreino.ipynb  # VersÃ£o notebook interativo
â”œâ”€â”€ requirements.txt # DependÃªncias 
â””â”€â”€ transformer_app.py # Web App Playground interativo

```

---

## ğŸ Como Instalar, Usar e Treinar um Modelo

```bash
# Criar e ativar o ambiente virtual
python -m venv myenv
.\myenv\Scripts\activate

# Instalar as DepedÃªncias
pip install torch streamlit numpy pillow matplotlib
ou
pip install -r requirements.txt

# Rodar app Playground 
cd transformer_app_playground
streamlit run main.py

# Rodar app Transformer Train
cd transformer_train
streamlit run train_transformer_app.py
ou
# Treinar modelo via terminal
python train_transformer.py
```

---

## ğŸ“Œ Transformer Train

O app Transformer Train Ã© responsÃ¡vel por **treinar um modelo Transformer simples** para tarefas de geraÃ§Ã£o de texto com base em pares de pergunta e resposta. Ele segue uma arquitetura compacta e educacional, ideal para fins de aprendizado, prototipagem e experimentaÃ§Ã£o.
<p align="center">
  <img src="./atencao.png" alt="Transformer Train" width="600"/>
</p>

<p align="center">
  <img src="./train.png" alt="Transformer Train" width="600"/>
</p>

### ğŸ›ï¸ Funcionalidades:

- **PrÃ©-processamento do dataset** (`textDataset.txt`).
- **DefiniÃ§Ã£o do modelo Transformer** com nÃºmero ajustÃ¡vel de camadas, cabeÃ§as, embeddings.
- **Treinamento supervisionado** com otimizaÃ§Ã£o via `Adam` e `cross_entropy`.
- **Salvamento dos pesos do modelo** e vocabulÃ¡rio treinado.
- **GeraÃ§Ã£o de texto** a partir de um prompt, usando top-k e top-p sampling.
- **GeraÃ§Ã£o de grÃ¡fico**: um grÃ¡fico com a curva de perda (loss) para facilitar a anÃ¡lise do desempenho do modelo.
- **Exibe o batch atual**: mostra os tokens numÃ©ricos e as palavras reais.
- **Gera um heatmap da atenÃ§Ã£o**: gera um heatmap da atenÃ§Ã£o da Ãºltima camada e primeira cabeÃ§a de atenÃ§Ã£o.
- **Exibe a loss**: Exibe a loss mÃ©dia a cada Ã©poca.
- **Atualiza o progresso batch**: Atualiza o progresso batch a batch em tempo real.

---

## ğŸ’¬ Playground Web App

O projeto inclui uma interface interativa desenvolvida com Streamlit chamada **Playground**, que permite explorar e testar o modelo treinado pelo Transformer Train diretamente no navegador.
O modelo treinado Ã© salvo no modelo_transformer.pt. Para usar o modelo treinado basta copiar o modelo_transformer.pt para a pasta transformer_app_playground.

<p align="center">
  <img src="./playground.png" alt="Playground" width="600"/>
</p>
<p align="center">
  <img src="./visualizacao.png" alt="Playground" width="600"/>
</p>
A interface Ã© intuitiva e oferece os seguintes recursos:

* Entrada de texto para perguntas abertas
* Sliders interativos para configurar os parÃ¢metros da geraÃ§Ã£o:

  * NÃºmero de palavras a gerar
  * Temperatura (controla a aleatoriedade)
  * Top-k (seleÃ§Ã£o dos k tokens mais provÃ¡veis)
  * Top-p (nucleus sampling, controle de diversidade)
* VisualizaÃ§Ã£o clara da resposta gerada
* ExibiÃ§Ã£o lateral do nÃºmero total de parÃ¢metros do modelo

Essa interface Ã© ideal para testes rÃ¡pidos e demonstraÃ§Ãµes, permitindo que estudantes e entusiastas compreendam na prÃ¡tica como os ajustes de parÃ¢metros afetam a saÃ­da do modelo.

---

## âœ¨ Exemplo de uso

```text
prompt: pergunta: quanto Ã© 2 mais 2? resposta:
output: 4 <eos>
```

---

## ğŸ“š ReferÃªncias

* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [The Annotated Transformer](http://nlp.seas.harvard.edu/annotated-transformer/)
* Baseado em prÃ¡ticas de modelos como GPT-2

---

## ğŸ“˜ CrÃ©ditos

ğŸ“˜ **Transformer Core** foi desenvolvido pelo Prof. [Fabio Santos](https://www.linkedin.com/in/fabio-santos-3706906/), com foco em ensino, prototipagem e experimentaÃ§Ã£o de modelos de linguagem baseados na arquitetura Transformer.

---

## âš ï¸ LicenÃ§a e Uso

Este projeto Ã© distribuÃ­do exclusivamente para **fins educacionais e nÃ£o comerciais**.  
O uso Ã© permitido para estudo, pesquisa e prototipagem, com a devida atribuiÃ§Ã£o de crÃ©dito ao autor.  
Qualquer uso comercial ou redistribuiÃ§Ã£o requer permissÃ£o prÃ©via por escrito.

LicenÃ§a adaptada da [MIT License](https://opensource.org/licenses/MIT).
