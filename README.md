# üß† Transformer Core

**Transformer Core** √© uma implementa√ß√£o educacional e compacta da arquitetura Transformer, projetada para tarefas de NLP como gera√ß√£o de texto e perguntas/respostas. Leve, interpret√°vel e pronta para personaliza√ß√£o, ela √© ideal para estudantes, entusiastas e experimentos locais.

<p align="center">
  <img src="./logo_minitransformer.png" alt="Logo Transformer Core" width="300"/>
</p>

---

## ‚ÑπÔ∏è O que √© um Transformer?

Transformer √© uma arquitetura de rede neural que mudou fundamentalmente a abordagem da Intelig√™ncia Artificial. Foi introduzida no artigo seminal "Attention is All You Need" em 2017 e desde ent√£o se tornou a principal arquitetura para modelos de deep learning, impulsionando modelos de gera√ß√£o de texto como o GPT da OpenAI, o Llama da Meta e o Gemini do Google. Al√©m do texto, o Transformer tamb√©m √© aplicado na gera√ß√£o de √°udio, reconhecimento de imagens, previs√£o de estrutura de prote√≠nas e at√© em jogos, demonstrando sua versatilidade em diversos dom√≠nios.

Fundamentalmente, modelos Transformers de gera√ß√£o de texto operam com base no princ√≠pio de previs√£o da pr√≥xima palavra: dado um prompt textual do usu√°rio, qual √© a pr√≥xima palavra mais prov√°vel? A inova√ß√£o central e o poder dos Transformers residem no uso do mecanismo de autoaten√ß√£o (self-attention), que permite processar sequ√™ncias inteiras e capturar depend√™ncias de longo alcance com mais efic√°cia do que arquiteturas anteriores.

A fam√≠lia GPT-2 √© um exemplo proeminente de Transformers para gera√ß√£o de texto. O Transformer Core se inspira nesses modelos e compartilha muitos dos mesmos componentes e princ√≠pios arquiteturais fundamentais encontrados nos modelos atuais de ponta, tornando-o ideal para aprendizado e compreens√£o b√°sica.

---

## üß¨ Arquitetura Transformer

Todo Transformer de gera√ß√£o de texto √© composto por tr√™s componentes principais:

* **Embedding**: A entrada textual √© dividida em unidades menores chamadas *tokens*, que podem ser palavras ou subpalavras. Esses tokens s√£o convertidos em vetores num√©ricos chamados *embeddings*, que capturam o significado sem√¢ntico das palavras.

* **Bloco Transformer**: √â o bloco fundamental do modelo que processa e transforma os dados de entrada. Cada bloco inclui:

  * **Mecanismo de Aten√ß√£o (Attention Mechanism)**: permite que os tokens se comuniquem entre si, capturando informa√ß√µes contextuais e rela√ß√µes entre palavras.
  * **Camada MLP (Multilayer Perceptron)**: uma rede feed-forward que opera em cada token de forma independente. Enquanto a aten√ß√£o roteia informa√ß√µes entre tokens, o MLP refina a representa√ß√£o de cada token.

* **Probabilidades de Sa√≠da**: As camadas finais lineares e softmax transformam os *embeddings* processados em probabilidades, permitindo que o modelo preveja o pr√≥ximo token na sequ√™ncia.


---

## üß± Arquitetura do Transformer Core

O `Transformer Core` √© composto pelos seguintes blocos principais:

* **`Embedding`**: embedding de palavras e posi√ß√µes
* **`TransformerBlock`**: bloco completo com LayerNorm, residual e aten√ß√£o + MLP
* **`MultiHeadSelfAttention`**: aten√ß√£o com proje√ß√£o conjunta QKV, m√°scara causal e concatena√ß√£o de cabe√ßas
* **`TransformerMLP`**: MLP com GELU e proje√ß√µes lineares
* **`SimpleTransformer`**: empilhamento de blocos, seguido por uma proje√ß√£o linear para logits

Exemplo de configura√ß√£o:

```python
model = SimpleTransformer(
    vocab_size=len(vocab),
    embed_dim=128,
    num_heads=8,
    num_layers=4
)
```
---

## üöÄ Recursos

* Arquitetura Transformer simplificada com PyTorch puro
* Camada de aten√ß√£o multi-cabe√ßa (multi-head self-attention)
* Conex√µes residuais com normaliza√ß√£o e MLP
* Proje√ß√µes integradas de Q, K e V
* Gera√ß√£o de texto com temperatura, top-k e top-p
* Dataset estilo QA baseado em texto plano

---

## üìÅ Estrutura do projeto

```bash
transformer_chatbot_project/
‚îú‚îÄ‚îÄ model.py                # Arquitetura do Transformer Core
‚îú‚îÄ‚îÄ train_transformer_v3.py  # Script de treino e gera√ß√£o
‚îú‚îÄ‚îÄ textDataset.txt        # Dados de treinamento com pares QA
‚îú‚îÄ‚îÄ modelo_transformer.pt  # Modelo treinado
‚îú‚îÄ‚îÄ vocab_transformer.json # Vocabul√°rio serializado
‚îî‚îÄ‚îÄ TransformerChatbotTreino.ipynb  # Vers√£o notebook interativo
```

---

## üèÅ Como usar

```bash
pip install torch streamlit numpy
ou
pip install -r requirements.txt

# Treinar modelo
python train_transformer.py

# Rodar app Streamlit (caso implementado)
streamlit run transformer_app.py
```

---

## üìå Par√¢metros t√≠picos

* Camadas: 4
* Cabe√ßas de aten√ß√£o: 8
* Embedding: 128
* Hidden MLP: 256
* Tokens m√°ximos por prompt: 100
* Par√¢metros totais: \~806.000

---

## ‚ú® Exemplo de uso

```text
prompt: pergunta: quanto √© 2 mais 2? resposta:
output: 4 <eos>
```

---

## üìö Refer√™ncias

* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
* Baseado em pr√°ticas de modelos como GPT-2

---

Desenvolvido com ‚ù§Ô∏è para aprendizado, prototipagem e experimenta√ß√£o.

> "Construa seu pr√≥prio modelo, compreenda cada aten√ß√£o."
