# ğŸ§  MiniTransformer

**MiniTransformer** Ã© uma implementaÃ§Ã£o educacional e compacta da arquitetura Transformer, projetada para tarefas de NLP como geraÃ§Ã£o de texto e perguntas/respostas. Leve, interpretÃ¡vel e pronta para personalizaÃ§Ã£o, ela Ã© ideal para estudantes, entusiastas e experimentos locais.

![Logo MiniTransformer](./logo_minitransformer.png)

---

## ğŸš€ Recursos
- Arquitetura Transformer simplificada com PyTorch puro
- Camada de atenÃ§Ã£o multi-cabeÃ§a (multi-head self-attention)
- ConexÃµes residuais com normalizaÃ§Ã£o e MLP
- ProjeÃ§Ãµes integradas de Q, K e V
- GeraÃ§Ã£o de texto com temperatura, top-k e top-p
- Dataset estilo QA baseado em texto plano

---

## ğŸ§± Arquitetura Interna
O `MiniTransformer` Ã© composto pelos seguintes blocos principais:

- **Embedding de palavras e posiÃ§Ãµes**
- **`MultiHeadSelfAttention`**: atenÃ§Ã£o com projeÃ§Ã£o conjunta QKV, mÃ¡scara causal e concatenaÃ§Ã£o de cabeÃ§as
- **`TransformerMLP`**: MLP com GELU e projeÃ§Ãµes lineares
- **`TransformerBlock`**: bloco completo com LayerNorm, residual e atenÃ§Ã£o + MLP
- **`SimpleTransformer`**: empilhamento de blocos, seguido por uma projeÃ§Ã£o linear para logits

Exemplo de configuraÃ§Ã£o:
```python
model = SimpleTransformer(
    vocab_size=len(vocab),
    embed_dim=128,
    num_heads=8,
    num_layers=4
)
```

---

## ğŸ“ Estrutura do projeto
```bash
transformer_chatbot_project/
â”œâ”€â”€ model.py                # Arquitetura do MiniTransformer
â”œâ”€â”€ train_transformer_v3.py  # Script de treino e geraÃ§Ã£o
â”œâ”€â”€ textDataset.txt        # Dados de treinamento com pares QA
â”œâ”€â”€ modelo_transformer.pt  # Modelo treinado
â”œâ”€â”€ vocab_transformer.json # VocabulÃ¡rio serializado
â””â”€â”€ TransformerChatbotTreino.ipynb  # VersÃ£o notebook interativo
```

---

## ğŸ Como usar

```bash
pip install torch streamlit numpy

# Treinar modelo
python train_transformer_v3.py

# Rodar app Streamlit (caso implementado)
streamlit run transformer_app.py
```

---

## ğŸ“Œ ParÃ¢metros tÃ­picos
- Camadas: 4
- CabeÃ§as de atenÃ§Ã£o: 8
- Embedding: 128
- Hidden MLP: 256
- Tokens mÃ¡ximos por prompt: 100
- ParÃ¢metros totais: ~806.000

---

## âœ¨ Exemplo de uso
```text
prompt: pergunta: quanto Ã© 2 mais 2? resposta:
output: 4 <eos>
```

---

## ğŸ“š ReferÃªncias
- [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)
- Baseado em prÃ¡ticas de modelos como GPT-2

---

Desenvolvido com â¤ï¸ para aprendizado, prototipagem e experimentaÃ§Ã£o.

> "Construa seu prÃ³prio modelo, compreenda cada atenÃ§Ã£o."
