{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7a2ee3d",
   "metadata": {},
   "source": [
    "# üí¨ Transformer Chatbot Treinamento\n",
    "Este notebook treina um mini modelo Transformer para responder perguntas simples com base em um dataset estilo FAQ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298a353b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import json\n",
    "from model import SimpleTransformer\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8c7f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0):\n",
    "    top_k = min(top_k, logits.size(-1))\n",
    "    if top_k > 0:\n",
    "        values, _ = torch.topk(logits, top_k)\n",
    "        min_values = values[:, -1].unsqueeze(-1)\n",
    "        logits = torch.where(logits < min_values, torch.full_like(logits, float('-inf')), logits)\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "        sorted_indices_to_remove[:, 0] = 0\n",
    "        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "        logits[0, indices_to_remove] = float('-inf')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d93a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, filepath, seq_len=20):\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        tokens = []\n",
    "        for line in lines:\n",
    "            line = line.strip().lower()\n",
    "            if not line: continue\n",
    "            tokens.extend(line.split() + ['<eos>'])\n",
    "        vocab = sorted(set(tokens))\n",
    "        self.token_to_id = {tok: i for i, tok in enumerate(vocab)}\n",
    "        self.id_to_token = {i: tok for tok, i in self.token_to_id.items()}\n",
    "        self.data = [self.token_to_id[tok] for tok in tokens]\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx:idx+self.seq_len])\n",
    "        y = torch.tensor(self.data[idx+1:idx+self.seq_len+1])\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30c74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = QADataset(\"textDataset.txt\", seq_len=10)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414fa311",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleTransformer(\n",
    "    vocab_size=len(dataset.token_to_id),\n",
    "    embed_dim=128,\n",
    "    num_heads=8,\n",
    "    num_layers=4\n",
    ")\n",
    "\n",
    "# Exibe total de par√¢metros\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Modelo criado com {total_params:,} par√¢metros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa675b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.train()\n",
    "model.to(device)\n",
    "for epoch in range(15):\n",
    "    total_loss = 0\n",
    "    for x, y in dataloader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits, _ = model(x)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"√âpoca {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbd996",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"modelo_transformer.pt\")\n",
    "with open(\"vocab_transformer.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset.token_to_id, f)\n",
    "print(\"Modelo e vocabul√°rio salvos com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b58403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "prompt = \"pergunta: qual a capital do brasil? resposta:\"\n",
    "tokens = prompt.lower().split()\n",
    "input_ids = torch.tensor([[dataset.token_to_id[t] for t in tokens]]).to(device)\n",
    "\n",
    "generated = input_ids\n",
    "temperature = 1.0\n",
    "top_k = 5\n",
    "top_p = 0.9\n",
    "max_tokens = 10\n",
    "\n",
    "for _ in range(max_tokens):\n",
    "    logits, _ = model(generated)\n",
    "    logits = logits[:, -1, :] / temperature\n",
    "    filtered_logits = top_k_top_p_filtering(logits.clone(), top_k=top_k, top_p=top_p)\n",
    "    probs = F.softmax(filtered_logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    generated = torch.cat([generated, next_token], dim=1)\n",
    "\n",
    "texto_final = \" \".join([dataset.id_to_token[i] for i in generated[0].tolist()])\n",
    "print(\"\\nTexto gerado:\")\n",
    "print(texto_final)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}